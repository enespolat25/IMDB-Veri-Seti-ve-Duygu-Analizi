{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4TmX3qB8Cqu",
        "colab_type": "text"
      },
      "source": [
        "## Duygu Analizi Nedir?\n",
        "Duygu Analizi ile, örneğin bir konuşmacının veya yazarın bir belgeye, etkileşime ya da olaya ilişkin tutumunu (örneğin duyarlılık) belirlemek istiyoruz. Bu nedenle, metnin anlaşılması gereken, altında yatan amacı öngörmek doğal bir dil işleme sorunudur. Duygu çoğunlukla olumlu, olumsuz ve tarafsız kategorilere ayrılır. Duygu Analizi'nin kullanılmasıyla, örneğin bir müşteri hakkında, onun hakkında yazdığı incelemeye dayanarak, bir ürün hakkındaki görüş ve tutumlarını tahmin etmek istiyoruz. Bu nedenle, Duygu Analizi, incelemeler, anketler, belgeler ve daha pek çok şeye yaygın olarak uygulanır.\n",
        "\n",
        "## İmdb Veri Kümesi\n",
        "İmdb duyarlılık sınıflandırma veri seti, pozitif (1) veya negatif (0) olarak etiketlenmiş imdb kullanıcılarının 50.000 film incelemesinden oluşur. İncelemeler önceden işlenir ve her biri tam sayı biçiminde bir sözcük dizini dizisi olarak kodlanır. İncelemelerdeki kelimeler, veri kümesi içindeki genel sıklıklarına göre dizine eklenir. Örneğin, “2” tamsayısı, verilerdeki ikinci en sık kullanılan sözcüğü kodlar. 50.000 inceleme, eğitim için 25.000 ve test için 25.000’e bölünmüştür. Veri seti, Stanford Üniversitesi araştırmacıları tarafından oluşturulmuştur ve 2011'de % 88.89 doğruluk elde ettikleri bir bildiri ile yayınlanmıştır. 2011'de “Bag of Words Meets Bags of Popcorn” adlı Kaggle yarışmasında da kullanılmıştır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYLwSVSU8o69",
        "colab_type": "text"
      },
      "source": [
        "### Gerekli Temel Kütüphaneler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErdmblLU9A9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e8105ca-db27-45ca-e3b8-66f2863f3595"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaFaUUh69FiY",
        "colab_type": "text"
      },
      "source": [
        "### Önce IMDB Veri setimizi indirelim daha sonra Train ve Test verilerimizi birleştirelim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCrRkprS9Wj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fF5Zmcr9ccX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "b80554ba-57c0-41ea-e361-5ec92cca7d67"
      },
      "source": [
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUMoVwzd9Wfr",
        "colab_type": "text"
      },
      "source": [
        "Yukarıdaki çıktıda veri kümesinin, incelemenin duyarlılığını temsil eden 0 veya 1 olmak üzere iki kategoride etiketlendiğini görebilirsiniz. Veri kümesinin tamamı 9998 benzersiz kelime içeriyor ve ortalama inceleme uzunluğu 233 kelime, standart 173 kelime sapma ile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBGnz1qr-5Z4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77ebac6a-6d3f-4601-c53e-adbb544b0776"
      },
      "source": [
        "print(\"Label:\", targets[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7JwOv9Y_Dq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9f09db73-3855-4420-9ff3-22c87c097b4f"
      },
      "source": [
        "print(data[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3fRHcJ9obT",
        "colab_type": "text"
      },
      "source": [
        "Yukarıda, pozitif olarak etiketlenen veri kümesinin ilk incelemesini görüyorsunuz (1). Aşağıdaki kod, sözlük eşleme kelime endekslerini orijinal kelimelere geri getirir, böylece bunları okuyabiliriz. Bilinmeyen her kelimeyi \"#\" ile değiştirir. Bunu get_word_index () işlevini kullanarak yapar ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR0Q-uCX_GDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "0379bfd8-192a-4278-cb21-70f73ea92657"
      },
      "source": [
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
        "print(decoded) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n",
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p5eQxOw93gG",
        "colab_type": "text"
      },
      "source": [
        "## Veri Hazırlama\n",
        "Şimdi verilerimizi hazırlama zamanı. Her incelemeyi vektörleştireceğiz ve tam olarak 10.000 sayı içerecek şekilde sıfırlarla dolduracağız. Bu, 10.000'den kısa her incelemeyi sıfırlarla doldurduğumuz anlamına gelir. Bunu yapıyoruz çünkü en büyük inceleme neredeyse o kadar uzun ve sinir ağımız için her girdi aynı boyutta olmalı. Ayrıca hedefleri yüzer hale getiriyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvSznSPy_KUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUekAgJC-B3K",
        "colab_type": "text"
      },
      "source": [
        "Şimdi verilerimizi bir eğitim ve test setine ayırdık. Eğitim seti 40.000 gözden geçirmeyi ve test seti 10.000'i içerecek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXGI5EVP_Pxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndyuknmF-Rc6",
        "colab_type": "text"
      },
      "source": [
        "## Model Oluşturma ve Eğitim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NSbwg9o_aHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "7923aa8b-850f-4b17-dc10-c8cb07a673ae"
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "# Input - Layer\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "# Hidden - Layers\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.summary()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 505,201\n",
            "Trainable params: 505,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MitmE2y_-d4Z",
        "colab_type": "text"
      },
      "source": [
        "binary_crossentropy ve Adam optimizer ile compile edelim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyDoD1IV_d0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqq2Ah2SAa8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "ee1e49e9-3337-4b24-8321-1eb1470f685f"
      },
      "source": [
        "results = model.fit(\n",
        " train_x, train_y,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (test_x, test_y)\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 6s 147us/step - loss: 0.4044 - acc: 0.8218 - val_loss: 0.2636 - val_acc: 0.8949\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 3s 74us/step - loss: 0.2122 - acc: 0.9184 - val_loss: 0.2598 - val_acc: 0.8948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km1gEgWo-q9m",
        "colab_type": "text"
      },
      "source": [
        "Modelimizi değerlendirme zamanı:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap2m-Wa5Adsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0d730b4a-6147-4d5e-8190-61e4c05ea130"
      },
      "source": [
        "print(np.mean(results.history[\"val_acc\"]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8948500022292136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_eSnRqN-xIR",
        "colab_type": "text"
      },
      "source": [
        "Bu basit modelle, başlangıçta bahsettiğim 2011 makalesinin doğruluğunu çoktan aştık. Hiperparametreleri ve katman sayısını denemekten çekinmeyin.\n",
        "### Faydalı olması dileğiyle"
      ]
    }
  ]
}